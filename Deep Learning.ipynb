{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DL_Project_Taki.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzOEFdlW0IE"
      },
      "source": [
        "**IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi5Nl997oha5"
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KedV9iL_SDrh"
      },
      "source": [
        "### Everything below will be copied to file named code.py\n",
        "%%writefile code.py\n",
        "\n",
        "def conv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, scope='conv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "\n",
        "        # d1,d2 = padding[i]\n",
        "        # size of ith dimension before padding : [d] and after padding [d1,d,d2]\n",
        "\n",
        "        padding = [[0, 0], [pad, pad], [pad, pad], [0, 0]]\n",
        "        if pad_type == 'zero' :\n",
        "            x = tf.pad(x, padding)\n",
        "        if pad_type == 'reflect' :\n",
        "            x = tf.pad(x, padding, mode='REFLECT')\n",
        "\n",
        "        x = tf.compat.v1.layers.conv2d(inputs=x, filters=channels,\n",
        "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                             kernel_regularizer=weight_regularizer,\n",
        "                             strides=stride, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "## Below script upgrades code.py file from tensorflow v1 to tensorflow v2\n",
        "!tf_upgrade_v2  --infile /content/StarGAN-Tensorflow/code.py --outfile /content/StarGAN-Tensorflow/code.py\n",
        "\n",
        "## A pop up will open with the content of code.py\n",
        "%pycat code.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEMjXElkOep1"
      },
      "source": [
        "#import tensorflow.contrib as tf_contrib\n",
        "\n",
        "\n",
        "# Xavier : tf_contrib.layers.xavier_initializer()\n",
        "# He : tf_contrib.layers.variance_scaling_initializer()\n",
        "# Normal : tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "# l2_decay : tf_contrib.layers.l2_regularizer(0.0001)\n",
        "\n",
        "weight_init = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
        "weight_regularizer = None\n",
        "\n",
        "##################################################################################\n",
        "# Layer\n",
        "##################################################################################\n",
        "\n",
        "def conv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, scope='conv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "\n",
        "        # d1,d2 = padding[i]\n",
        "        # size of ith dimension before padding : [d] and after padding [d1,d,d2]\n",
        "\n",
        "        padding = [[0, 0], [pad, pad], [pad, pad], [0, 0]]\n",
        "        if pad_type == 'zero' :\n",
        "            x = tf.pad(x, padding)\n",
        "        if pad_type == 'reflect' :\n",
        "            x = tf.pad(x, padding, mode='REFLECT')\n",
        "\n",
        "        x = tf.compat.v1.layers.conv2d(inputs=x, filters=channels,\n",
        "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                             kernel_regularizer=weight_regularizer,\n",
        "                             strides=stride, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def conv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, scope='conv_0'):\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "\n",
        "        # d1,d2 = padding[i]\n",
        "        # size of ith dimension before padding : [d] and after padding [d1,d,d2]\n",
        "\n",
        "        padding = [[0, 0], [pad, pad], [pad, pad], [0, 0]]\n",
        "        if pad_type == 'zero' :\n",
        "            x = tf.pad(tensor=x, paddings=padding)\n",
        "        if pad_type == 'reflect' :\n",
        "            x = tf.pad(tensor=x, paddings=padding, mode='REFLECT')\n",
        "\n",
        "        x = tf.compat.v1.layers.conv2d(inputs=x, filters=channels,\n",
        "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                             kernel_regularizer=weight_regularizer,\n",
        "                             strides=stride, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def deconv(x, channels, kernel=4, stride=2, use_bias=True, scope='deconv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = tf.layers.conv2d_transpose(inputs=x, filters=channels,\n",
        "                                       kernel_size=kernel, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer,\n",
        "                                       strides=stride, padding='SAME', use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "def flatten(x) :\n",
        "    return tf.layers.flatten(x)\n",
        "\n",
        "##################################################################################\n",
        "# Residual-block\n",
        "##################################################################################\n",
        "\n",
        "def resblock(x_init, channels, use_bias=True, scope='resblock'):\n",
        "    with tf.variable_scope(scope):\n",
        "        with tf.variable_scope('res1'):\n",
        "            x = conv(x_init, channels, kernel=3, stride=1, pad=1, use_bias=use_bias)\n",
        "            x = instance_norm(x)\n",
        "            x = relu(x)\n",
        "\n",
        "        with tf.variable_scope('res2'):\n",
        "            x = conv(x, channels, kernel=3, stride=1, pad=1, use_bias=use_bias)\n",
        "            x = instance_norm(x)\n",
        "\n",
        "        return x + x_init\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Activation function\n",
        "##################################################################################\n",
        "\n",
        "def lrelu(x, alpha=0.2):\n",
        "    return tf.nn.leaky_relu(x, alpha)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return tf.tanh(x)\n",
        "\n",
        "##################################################################################\n",
        "# Normalization function\n",
        "##################################################################################\n",
        "\n",
        "def instance_norm(x, scope='instance_norm'):\n",
        "    return tf.contrib.layers.instance_norm(x,\n",
        "                                           epsilon=1e-05,\n",
        "                                           center=True, scale=True,\n",
        "                                           scope=scope)\n",
        "\n",
        "##################################################################################\n",
        "# Loss function\n",
        "##################################################################################\n",
        "\n",
        "def discriminator_loss(loss_func, real, fake):\n",
        "    real_loss = 0\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        real_loss = -tf.reduce_mean(real)\n",
        "        fake_loss = tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        real_loss = tf.reduce_mean(tf.squared_difference(real, 1.0))\n",
        "        fake_loss = tf.reduce_mean(tf.square(fake))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        real_loss = tf.reduce_mean(relu(1.0 - real))\n",
        "        fake_loss = tf.reduce_mean(relu(1.0 + fake))\n",
        "\n",
        "    loss = real_loss + fake_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def generator_loss(loss_func, fake):\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        fake_loss = tf.reduce_mean(tf.squared_difference(fake, 1.0))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "\n",
        "    loss = fake_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def classification_loss(logit, label) :\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label, logits=logit))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def L1_loss(x, y):\n",
        "    loss = tf.reduce_mean(tf.abs(x - y))\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRXRN12o29u"
      },
      "source": [
        "%pycat StarGAN.py\n",
        "\n",
        "from ops import *\n",
        "from utils import *\n",
        "import time\n",
        "from tensorflow.contrib.data import prefetch_to_device, shuffle_and_repeat, map_and_batch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "class StarGAN(object) :\n",
        "    def __init__(self, sess, args):\n",
        "        self.model_name = 'StarGAN'\n",
        "        self.sess = sess\n",
        "        self.checkpoint_dir = args.checkpoint_dir\n",
        "        self.sample_dir = args.sample_dir\n",
        "        self.result_dir = args.result_dir\n",
        "        self.log_dir = args.log_dir\n",
        "        self.dataset_name = args.dataset\n",
        "        self.dataset_path = os.path.join('./dataset', self.dataset_name)\n",
        "        self.augment_flag = args.augment_flag\n",
        "\n",
        "        self.epoch = args.epoch\n",
        "        self.iteration = args.iteration\n",
        "        self.decay_flag = args.decay_flag\n",
        "        self.decay_epoch = args.decay_epoch\n",
        "\n",
        "        self.gan_type = args.gan_type\n",
        "\n",
        "        self.batch_size = args.batch_size\n",
        "        self.print_freq = args.print_freq\n",
        "        self.save_freq = args.save_freq\n",
        "\n",
        "        self.init_lr = args.lr\n",
        "        self.ch = args.ch\n",
        "        self.selected_attrs = args.selected_attrs\n",
        "        self.custom_label = np.expand_dims(args.custom_label, axis=0)\n",
        "        self.c_dim = len(self.selected_attrs)\n",
        "\n",
        "        \"\"\" Weight \"\"\"\n",
        "        self.adv_weight = args.adv_weight\n",
        "        self.rec_weight = args.rec_weight\n",
        "        self.cls_weight = args.cls_weight\n",
        "        self.ld = args.ld\n",
        "\n",
        "        \"\"\" Generator \"\"\"\n",
        "        self.n_res = args.n_res\n",
        "\n",
        "        \"\"\" Discriminator \"\"\"\n",
        "        self.n_dis = args.n_dis\n",
        "        self.n_critic = args.n_critic\n",
        "\n",
        "        self.img_size = args.img_size\n",
        "        self.img_ch = args.img_ch\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Information #####\")\n",
        "        print(\"# gan type : \", self.gan_type)\n",
        "        print(\"# selected_attrs : \", self.selected_attrs)\n",
        "        print(\"# dataset : \", self.dataset_name)\n",
        "        print(\"# batch_size : \", self.batch_size)\n",
        "        print(\"# epoch : \", self.epoch)\n",
        "        print(\"# iteration per epoch : \", self.iteration)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Generator #####\")\n",
        "        print(\"# residual blocks : \", self.n_res)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Discriminator #####\")\n",
        "        print(\"# discriminator layer : \", self.n_dis)\n",
        "        print(\"# the number of critic : \", self.n_critic)\n",
        "\n",
        "    ##################################################################################\n",
        "    # Generator\n",
        "    ##################################################################################\n",
        "\n",
        "    def generator(self, x_init, c, reuse=False, scope=\"generator\"):\n",
        "        channel = self.ch\n",
        "        c = tf.cast(tf.reshape(c, shape=[-1, 1, 1, c.shape[-1]]), tf.float32)\n",
        "        c = tf.tile(c, [1, x_init.shape[1], x_init.shape[2], 1])\n",
        "        x = tf.concat([x_init, c], axis=-1)\n",
        "\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            x = conv(x, channel, kernel=7, stride=1, pad=3, use_bias=False, scope='conv')\n",
        "            x = instance_norm(x, scope='ins_norm')\n",
        "            x = relu(x)\n",
        "\n",
        "            # Down-Sampling\n",
        "            for i in range(2) :\n",
        "                x = conv(x, channel*2, kernel=4, stride=2, pad=1, use_bias=False, scope='conv_'+str(i))\n",
        "                x = instance_norm(x, scope='down_ins_norm_'+str(i))\n",
        "                x = relu(x)\n",
        "\n",
        "                channel = channel * 2\n",
        "\n",
        "            # Bottleneck\n",
        "            for i in range(self.n_res):\n",
        "                x = resblock(x, channel, use_bias=False, scope='resblock_' + str(i))\n",
        "\n",
        "            # Up-Sampling\n",
        "            for i in range(2) :\n",
        "                x = deconv(x, channel//2, kernel=4, stride=2, use_bias=False, scope='deconv_'+str(i))\n",
        "                x = instance_norm(x, scope='up_ins_norm'+str(i))\n",
        "                x = relu(x)\n",
        "\n",
        "                channel = channel // 2\n",
        "\n",
        "\n",
        "            x = conv(x, channels=3, kernel=7, stride=1, pad=3, use_bias=False, scope='G_logit')\n",
        "            x = tanh(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    ##################################################################################\n",
        "    # Discriminator\n",
        "    ##################################################################################\n",
        "\n",
        "    def discriminator(self, x_init, reuse=False, scope=\"discriminator\"):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            channel = self.ch\n",
        "            x = conv(x_init, channel, kernel=4, stride=2, pad=1, use_bias=True, scope='conv_0')\n",
        "            x = lrelu(x, 0.01)\n",
        "\n",
        "            for i in range(1, self.n_dis):\n",
        "                x = conv(x, channel * 2, kernel=4, stride=2, pad=1, use_bias=True, scope='conv_' + str(i))\n",
        "                x = lrelu(x, 0.01)\n",
        "\n",
        "                channel = channel * 2\n",
        "\n",
        "            c_kernel = int(self.img_size / np.power(2, self.n_dis))\n",
        "\n",
        "            logit = conv(x, channels=1, kernel=3, stride=1, pad=1, use_bias=False, scope='D_logit')\n",
        "            c = conv(x, channels=self.c_dim, kernel=c_kernel, stride=1, use_bias=False, scope='D_label')\n",
        "            c = tf.reshape(c, shape=[-1, self.c_dim])\n",
        "\n",
        "            return logit, c\n",
        "\n",
        "    ##################################################################################\n",
        "    # Model\n",
        "    ##################################################################################\n",
        "\n",
        "    def gradient_panalty(self, real, fake, scope=\"discriminator\"):\n",
        "        if self.gan_type == 'dragan' :\n",
        "            shape = tf.shape(real)\n",
        "            eps = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n",
        "            x_mean, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n",
        "            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n",
        "            noise = 0.5 * x_std * eps  # delta in paper\n",
        "\n",
        "            # Author suggested U[0,1] in original paper, but he admitted it is bug in github\n",
        "            # (https://github.com/kodalinaveen3/DRAGAN). It should be two-sided.\n",
        "\n",
        "            alpha = tf.random_uniform(shape=[shape[0], 1, 1, 1], minval=-1., maxval=1.)\n",
        "            interpolated = tf.clip_by_value(real + alpha * noise, -1., 1.)  # x_hat should be in the space of X\n",
        "\n",
        "        else :\n",
        "            alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n",
        "            interpolated = alpha*real + (1. - alpha)*fake\n",
        "\n",
        "        logit, _ = self.discriminator(interpolated, reuse=True, scope=scope)\n",
        "\n",
        "\n",
        "        GP = 0\n",
        "\n",
        "        grad = tf.gradients(logit, interpolated)[0] # gradient of D(interpolated)\n",
        "        grad_norm = tf.norm(flatten(grad), axis=1) # l2 norm\n",
        "\n",
        "        # WGAN - LP\n",
        "        if self.gan_type == 'wgan-lp' :\n",
        "            GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n",
        "\n",
        "        elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\n",
        "            GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n",
        "\n",
        "        return GP\n",
        "\n",
        "    def build_model(self):\n",
        "        self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "        \"\"\" Input Image\"\"\"\n",
        "        Image_data_class = ImageData(load_size=self.img_size, channels=self.img_ch, data_path=self.dataset_path, selected_attrs=self.selected_attrs, augment_flag=self.augment_flag)\n",
        "        Image_data_class.preprocess()\n",
        "\n",
        "        train_dataset_num = len(Image_data_class.train_dataset)\n",
        "        test_dataset_num = len(Image_data_class.test_dataset)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((Image_data_class.train_dataset, Image_data_class.train_dataset_label, Image_data_class.train_dataset_fix_label))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((Image_data_class.test_dataset, Image_data_class.test_dataset_label, Image_data_class.test_dataset_fix_label))\n",
        "\n",
        "        gpu_device = '/gpu:0'\n",
        "        train_dataset = train_dataset.\\\n",
        "            apply(shuffle_and_repeat(train_dataset_num)).\\\n",
        "            apply(map_and_batch(Image_data_class.image_processing, self.batch_size, num_parallel_batches=8, drop_remainder=True)).\\\n",
        "            apply(prefetch_to_device(gpu_device, self.batch_size))\n",
        "\n",
        "        test_dataset = test_dataset.\\\n",
        "            apply(shuffle_and_repeat(test_dataset_num)).\\\n",
        "            apply(map_and_batch(Image_data_class.image_processing, self.batch_size, num_parallel_batches=8, drop_remainder=True)).\\\n",
        "            apply(prefetch_to_device(gpu_device, self.batch_size))\n",
        "\n",
        "        train_dataset_iterator = train_dataset.make_one_shot_iterator()\n",
        "        test_dataset_iterator = test_dataset.make_one_shot_iterator()\n",
        "\n",
        "\n",
        "        self.x_real, label_org, label_fix_list = train_dataset_iterator.get_next() # Input image / Original domain labels\n",
        "        label_trg = tf.random_shuffle(label_org) # Target domain labels\n",
        "        label_fix_list = tf.transpose(label_fix_list, perm=[1, 0, 2])\n",
        "\n",
        "        self.x_test, test_label_org, test_label_fix_list = test_dataset_iterator.get_next()  # Input image / Original domain labels\n",
        "        test_label_fix_list = tf.transpose(test_label_fix_list, perm=[1, 0, 2])\n",
        "\n",
        "        self.custom_image = tf.placeholder(tf.float32, [1, self.img_size, self.img_size, self.img_ch], name='custom_image') # Custom Image\n",
        "        custom_label_fix_list = tf.transpose(create_labels(self.custom_label, self.selected_attrs), perm=[1, 0, 2])\n",
        "\n",
        "        \"\"\" Define Generator, Discriminator \"\"\"\n",
        "        x_fake = self.generator(self.x_real, label_trg) # real a\n",
        "        x_recon = self.generator(x_fake, label_org, reuse=True) # real b\n",
        "\n",
        "        real_logit, real_cls = self.discriminator(self.x_real)\n",
        "        fake_logit, fake_cls = self.discriminator(x_fake, reuse=True)\n",
        "\n",
        "\n",
        "        \"\"\" Define Loss \"\"\"\n",
        "        if self.gan_type.__contains__('wgan') or self.gan_type == 'dragan' :\n",
        "            GP = self.gradient_panalty(real=self.x_real, fake=x_fake)\n",
        "        else :\n",
        "            GP = 0\n",
        "\n",
        "        g_adv_loss = generator_loss(loss_func=self.gan_type, fake=fake_logit)\n",
        "        g_cls_loss = classification_loss(logit=fake_cls, label=label_trg)\n",
        "        g_rec_loss = L1_loss(self.x_real, x_recon)\n",
        "\n",
        "        d_adv_loss = discriminator_loss(loss_func=self.gan_type, real=real_logit, fake=fake_logit) + GP\n",
        "        d_cls_loss = classification_loss(logit=real_cls, label=label_org)\n",
        "\n",
        "        self.d_loss = self.adv_weight * d_adv_loss + self.cls_weight * d_cls_loss\n",
        "        self.g_loss = self.adv_weight * g_adv_loss + self.cls_weight * g_cls_loss + self.rec_weight * g_rec_loss\n",
        "\n",
        "\n",
        "        \"\"\" Result Image \"\"\"\n",
        "        self.x_fake_list = tf.map_fn(lambda x : self.generator(self.x_real, x, reuse=True), label_fix_list, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        \"\"\" Test Image \"\"\"\n",
        "        self.x_test_fake_list = tf.map_fn(lambda x : self.generator(self.x_test, x, reuse=True), test_label_fix_list, dtype=tf.float32)\n",
        "        self.custom_fake_image = tf.map_fn(lambda x : self.generator(self.custom_image, x, reuse=True), custom_label_fix_list, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "        t_vars = tf.trainable_variables()\n",
        "        G_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "\n",
        "        self.g_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.g_loss, var_list=G_vars)\n",
        "        self.d_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.d_loss, var_list=D_vars)\n",
        "\n",
        "\n",
        "        \"\"\"\" Summary \"\"\"\n",
        "        self.Generator_loss = tf.summary.scalar(\"Generator_loss\", self.g_loss)\n",
        "        self.Discriminator_loss = tf.summary.scalar(\"Discriminator_loss\", self.d_loss)\n",
        "\n",
        "        self.g_adv_loss = tf.summary.scalar(\"g_adv_loss\", g_adv_loss)\n",
        "        self.g_cls_loss = tf.summary.scalar(\"g_cls_loss\", g_cls_loss)\n",
        "        self.g_rec_loss = tf.summary.scalar(\"g_rec_loss\", g_rec_loss)\n",
        "\n",
        "        self.d_adv_loss = tf.summary.scalar(\"d_adv_loss\", d_adv_loss)\n",
        "        self.d_cls_loss = tf.summary.scalar(\"d_cls_loss\", d_cls_loss)\n",
        "\n",
        "        self.g_summary_loss = tf.summary.merge([self.Generator_loss, self.g_adv_loss, self.g_cls_loss, self.g_rec_loss])\n",
        "        self.d_summary_loss = tf.summary.merge([self.Discriminator_loss, self.d_adv_loss, self.d_cls_loss])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
        "            counter = checkpoint_counter\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        self.sample_dir = os.path.join(self.sample_dir, self.model_dir)\n",
        "        check_folder(self.sample_dir)\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "        past_g_loss = -1.\n",
        "        lr = self.init_lr\n",
        "        for epoch in range(start_epoch, self.epoch):\n",
        "            if self.decay_flag :\n",
        "                lr = self.init_lr if epoch < self.decay_epoch else self.init_lr * (self.epoch - epoch) / (self.epoch - self.decay_epoch) # linear decay\n",
        "\n",
        "            for idx in range(start_batch_id, self.iteration):\n",
        "                train_feed_dict = {\n",
        "                    self.lr : lr\n",
        "                }\n",
        "\n",
        "                # Update D\n",
        "                _, d_loss, summary_str = self.sess.run([self.d_optimizer, self.d_loss, self.d_summary_loss], feed_dict = train_feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # Update G\n",
        "                g_loss = None\n",
        "                if (counter - 1) % self.n_critic == 0 :\n",
        "                    real_images, fake_images, _, g_loss, summary_str = self.sess.run([self.x_real, self.x_fake_list, self.g_optimizer, self.g_loss, self.g_summary_loss], feed_dict = train_feed_dict)\n",
        "                    self.writer.add_summary(summary_str, counter)\n",
        "                    past_g_loss = g_loss\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "                if g_loss == None :\n",
        "                    g_loss = past_g_loss\n",
        "\n",
        "                print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss))\n",
        "\n",
        "                if np.mod(idx+1, self.print_freq) == 0 :\n",
        "                    real_image = np.expand_dims(real_images[0], axis=0)\n",
        "                    fake_image = np.transpose(fake_images, axes=[1, 0, 2, 3, 4])[0] # [bs, c_dim, h, w, ch]\n",
        "\n",
        "                    save_images(real_image, [1, 1],\n",
        "                                './{}/real_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\n",
        "\n",
        "                    save_images(fake_image, [1, self.c_dim],\n",
        "                                './{}/fake_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\n",
        "\n",
        "                if np.mod(idx + 1, self.save_freq) == 0:\n",
        "                    self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model for final step\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        n_res = str(self.n_res) + 'resblock'\n",
        "        n_dis = str(self.n_dis) + 'dis'\n",
        "\n",
        "        return \"{}_{}_{}_{}_{}\".format(self.model_name, self.dataset_name,\n",
        "                                       self.gan_type,\n",
        "                                       n_res, n_dis)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        import re\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0\n",
        "\n",
        "    def test(self):\n",
        "        tf.global_variables_initializer().run()\n",
        "        test_path = os.path.join(self.dataset_path, 'test')\n",
        "        check_folder(test_path)\n",
        "        test_files = glob(os.path.join(test_path, '*.*'))\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        self.result_dir = os.path.join(self.result_dir, self.model_dir)\n",
        "        check_folder(self.result_dir)\n",
        "\n",
        "        image_folder = os.path.join(self.result_dir, 'images')\n",
        "        check_folder(image_folder)\n",
        "\n",
        "        if could_load :\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else :\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # write html for visual comparison\n",
        "        index_path = os.path.join(self.result_dir, 'index.html')\n",
        "        index = open(index_path, 'w')\n",
        "        index.write(\"<html><body><table><tr>\")\n",
        "        index.write(\"<th>name</th><th>input</th><th>output</th></tr>\")\n",
        "\n",
        "        # Custom Image\n",
        "        for sample_file in test_files:\n",
        "            print(\"Processing image: \" + sample_file)\n",
        "            sample_image = np.asarray(load_test_data(sample_file, size=self.img_size))\n",
        "            image_path = os.path.join(image_folder, '{}'.format(os.path.basename(sample_file)))\n",
        "\n",
        "            fake_image = self.sess.run(self.custom_fake_image, feed_dict = {self.custom_image : sample_image})\n",
        "            fake_image = np.transpose(fake_image, axes=[1, 0, 2, 3, 4])[0]\n",
        "            save_images(fake_image, [1, self.c_dim], image_path)\n",
        "\n",
        "            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\n",
        "            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (sample_file if os.path.isabs(sample_file) else (\n",
        "                        '../..' + os.path.sep + sample_file), self.img_size, self.img_size))\n",
        "\n",
        "            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (image_path if os.path.isabs(image_path) else (\n",
        "                        '../..' + os.path.sep + image_path), self.img_size * self.c_dim, self.img_size))\n",
        "            index.write(\"</tr>\")\n",
        "\n",
        "        # CelebA\n",
        "        real_images, fake_images = self.sess.run([self.x_test, self.x_test_fake_list])\n",
        "        fake_images = np.transpose(fake_images, axes=[1, 0, 2, 3, 4])\n",
        "\n",
        "        for i in range(len(real_images)) :\n",
        "            print(\"{} / {}\".format(i, len(real_images)))\n",
        "            real_path = os.path.join(image_folder, 'real_{}.png'.format(i))\n",
        "            fake_path = os.path.join(image_folder, 'fake_{}.png'.format(i))\n",
        "\n",
        "            real_image = np.expand_dims(real_images[i], axis=0)\n",
        "            fake_image = fake_images[i]\n",
        "            save_images(real_image, [1, 1], real_path)\n",
        "            save_images(fake_image, [1, self.c_dim], fake_path)\n",
        "\n",
        "            index.write(\"<td>%s</td>\" % os.path.basename(real_path))\n",
        "            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (real_path if os.path.isabs(real_path) else (\n",
        "                '../..' + os.path.sep + real_path), self.img_size, self.img_size))\n",
        "\n",
        "            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (fake_path if os.path.isabs(fake_path) else (\n",
        "                '../..' + os.path.sep + fake_path), self.img_size * self.c_dim, self.img_size))\n",
        "            index.write(\"</tr>\")\n",
        "\n",
        "        index.close()\n",
        "parameter_s, hint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrbyJGqZ4vx1"
      },
      "source": [
        "%pycat StarGAN.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucrV5lGpbrn"
      },
      "source": [
        "%pycat main.py\n",
        "\n",
        "from StarGAN import StarGAN\n",
        "import argparse\n",
        "from utils import *\n",
        "\n",
        "\"\"\"parsing and configuration\"\"\"\n",
        "def parse_args():\n",
        "    desc = \"Tensorflow implementation of StarGAN\"\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('--phase', type=str, default='train', help='train or test ?')\n",
        "    parser.add_argument('--dataset', type=str, default='celebA', help='dataset_name')\n",
        "\n",
        "    parser.add_argument('--epoch', type=int, default=20, help='The number of epochs to run')\n",
        "    parser.add_argument('--iteration', type=int, default=10000, help='The number of training iterations')\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='The size of batch size')\n",
        "    parser.add_argument('--print_freq', type=int, default=1000, help='The number of image_print_freq')\n",
        "    parser.add_argument('--save_freq', type=int, default=1000, help='The number of ckpt_save_freq')\n",
        "    parser.add_argument('--decay_flag', type=str2bool, default=True, help='The decay_flag')\n",
        "    parser.add_argument('--decay_epoch', type=int, default=10, help='decay epoch')\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, help='The learning rate')\n",
        "    parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')\n",
        "    parser.add_argument('--adv_weight', type=float, default=1, help='Weight about GAN')\n",
        "    parser.add_argument('--rec_weight', type=float, default=10, help='Weight about Reconstruction')\n",
        "    parser.add_argument('--cls_weight', type=float, default=10, help='Weight about Classification')\n",
        "\n",
        "    parser.add_argument('--gan_type', type=str, default='wgan-gp', help='gan / lsgan / wgan-gp / wgan-lp / dragan / hinge')\n",
        "    parser.add_argument('--selected_attrs', type=str, nargs='+', help='selected attributes for the CelebA dataset',\n",
        "                        default=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'])\n",
        "\n",
        "    parser.add_argument('--custom_label', type=int, nargs='+', help='custom label about selected attributes',\n",
        "                        default=[1, 0, 0, 0, 0])\n",
        "    # If your image is \"Young, Man, Black Hair\" = [1, 0, 0, 1, 1]\n",
        "\n",
        "    parser.add_argument('--ch', type=int, default=64, help='base channel number per layer')\n",
        "    parser.add_argument('--n_res', type=int, default=6, help='The number of resblock')\n",
        "    parser.add_argument('--n_dis', type=int, default=6, help='The number of discriminator layer')\n",
        "    parser.add_argument('--n_critic', type=int, default=5, help='The number of critic')\n",
        "\n",
        "    parser.add_argument('--img_size', type=int, default=128, help='The size of image')\n",
        "    parser.add_argument('--img_ch', type=int, default=3, help='The size of image channel')\n",
        "    parser.add_argument('--augment_flag', type=str2bool, default=True, help='Image augmentation use or not')\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n",
        "                        help='Directory name to save the checkpoints')\n",
        "    parser.add_argument('--result_dir', type=str, default='results',\n",
        "                        help='Directory name to save the generated images')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                        help='Directory name to save training logs')\n",
        "    parser.add_argument('--sample_dir', type=str, default='samples',\n",
        "                        help='Directory name to save the samples on training')\n",
        "\n",
        "    return check_args(parser.parse_args())\n",
        "\n",
        "\"\"\"checking arguments\"\"\"\n",
        "def check_args(args):\n",
        "    # --checkpoint_dir\n",
        "    check_folder(args.checkpoint_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.result_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.log_dir)\n",
        "\n",
        "    # --sample_dir\n",
        "    check_folder(args.sample_dir)\n",
        "\n",
        "    # --epoch\n",
        "    try:\n",
        "        assert args.epoch >= 1\n",
        "    except:\n",
        "        print('number of epochs must be larger than or equal to one')\n",
        "\n",
        "    # --batch_size\n",
        "    try:\n",
        "        assert args.batch_size >= 1\n",
        "    except:\n",
        "        print('batch size must be larger than or equal to one')\n",
        "    return args\n",
        "\n",
        "\"\"\"main\"\"\"\n",
        "def main():\n",
        "    # parse arguments\n",
        "    args = parse_args()\n",
        "    if args is None:\n",
        "      exit()\n",
        "\n",
        "    # open session\n",
        "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        gan = StarGAN(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        gan.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        if args.phase == 'train' :\n",
        "            gan.train()\n",
        "            print(\" [*] Training finished!\")\n",
        "\n",
        "        if args.phase == 'test' :\n",
        "            gan.test()\n",
        "            print(\" [*] Test finished!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "parameter_s, hint"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}